{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a013d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Adjust import path to import turnout models\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e883ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import hashlib\n",
    "import itertools as it\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "import shutil\n",
    "import traceback\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import salk_turnout_models as tm\n",
    "from time import perf_counter\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d3822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', module='arviz', message='invalid value encountered in scalar divide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02128abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = logging.getLogger('synth-data-models')\n",
    "LOGGER.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b0b700",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e6a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_margins(pdf, columns, outcome):\n",
    "    if columns:\n",
    "        return (pdf.groupby(columns, observed=False)[outcome].value_counts() / len(pdf)).rename('proportion')\n",
    "    else:\n",
    "        return (pdf[outcome].value_counts() / len(pdf)).rename('proportion')\n",
    "\n",
    "def df_turnout(pdf, columns, outcome):\n",
    "    if columns:\n",
    "        groups = pdf.groupby(columns, observed=True)\n",
    "        return (groups[outcome].value_counts() / groups[outcome].size()).rename('proportion')\n",
    "    else:\n",
    "        return (pdf[outcome].value_counts() / len(pdf)).rename('proportion')\n",
    "\n",
    "def cell_margins(df, cell_cols, margin_cols, outcome):\n",
    "    avg_df = df.groupby(cell_cols)[['N', 'N_census']].mean().reset_index()\n",
    "\n",
    "    if margin_cols:\n",
    "        grouped_df = avg_df.groupby(margin_cols)[['N', 'N_census']].sum().reset_index()\n",
    "    else:\n",
    "        grouped_df = pd.DataFrame({'N': avg_df['N'].sum(), 'N_census': avg_df['N_census'].sum()}, index=[0])\n",
    "\n",
    "    yes_df = grouped_df.copy()\n",
    "    yes_df['proportion'] = yes_df['N'] / yes_df['N_census'].sum()\n",
    "    yes_df[outcome] = 'Yes'\n",
    "\n",
    "    no_df = grouped_df.copy()\n",
    "    no_df['proportion'] = (no_df['N_census'] - no_df['N']) / no_df['N_census'].sum()\n",
    "    no_df[outcome] = 'No'\n",
    "\n",
    "    return pd.concat([yes_df, no_df])[margin_cols + [outcome, 'proportion']].set_index(margin_cols + [outcome])\n",
    "\n",
    "def cell_turnout(df, cell_cols, margin_cols, outcome):\n",
    "    avg_df = df.groupby(cell_cols)[['N', 'N_census']].mean().reset_index()\n",
    "\n",
    "    if margin_cols:\n",
    "        grouped_df = avg_df.groupby(margin_cols)[['N', 'N_census']].sum().reset_index()\n",
    "    else:\n",
    "        grouped_df = pd.DataFrame({'N': avg_df['N'].sum(), 'N_census': avg_df['N_census'].sum()}, index=[0])\n",
    "\n",
    "    yes_df = grouped_df.copy()\n",
    "    yes_df['proportion'] = yes_df['N'] / yes_df['N_census']\n",
    "    yes_df[outcome] = 'Yes'\n",
    "\n",
    "    no_df = grouped_df.copy()\n",
    "    no_df['proportion'] = (no_df['N_census'] - no_df['N']) / no_df['N_census']\n",
    "    no_df[outcome] = 'No'\n",
    "\n",
    "    return pd.concat([yes_df, no_df])[margin_cols + [outcome, 'proportion']].set_index(margin_cols + [outcome])\n",
    "\n",
    "def kl_divergence(margins_df, epsilon=1e-10):\n",
    "    # Clip values to avoid zero division and log(0)\n",
    "    p = np.clip(margins_df['proportion_pop'].values.flatten(), epsilon, 1)\n",
    "    q = np.clip(margins_df['proportion_mod'].values.flatten(), epsilon, 1)\n",
    "    return np.sum(p * np.log(p / q)).item()\n",
    "\n",
    "def em_distance(margins_df, epsilon=1e-10):\n",
    "    # Clip values to avoid zero division and log(0)\n",
    "    p = np.clip(margins_df['proportion_pop'].values.flatten(), epsilon, 1)\n",
    "    q = np.clip(margins_df['proportion_mod'].values.flatten(), epsilon, 1)\n",
    "    return np.abs(p - q).sum().item() / 2\n",
    "\n",
    "def get_distances(pop_df, mod_df, columns, outcome):\n",
    "    pop_margins = df_margins(pop_df, columns, 'voting_intent').reset_index()\n",
    "    mod_margins = cell_margins(mod_df, columns, columns, 'voting_intent').reset_index()\n",
    "\n",
    "    margins_df = pd.merge(pop_margins, mod_margins, on=columns + [outcome], how='outer', suffixes=('_pop', '_mod')).fillna(0)\n",
    "\n",
    "    emd = em_distance(margins_df)\n",
    "    kld = kl_divergence(margins_df)\n",
    "\n",
    "    emd_1d = np.array([em_distance(margins_df.groupby([col, outcome])[['proportion_pop', 'proportion_mod']].sum()) for col in columns])\n",
    "    kld_1d = np.array([kl_divergence(margins_df.groupby([col, outcome])[['proportion_pop', 'proportion_mod']].sum()) for col in columns])\n",
    "\n",
    "    emd_2d = np.array([em_distance(margins_df.groupby([c1, c2, outcome])[['proportion_pop', 'proportion_mod']].sum()) for c1, c2 in it.combinations(columns, 2)])\n",
    "    kld_2d = np.array([kl_divergence(margins_df.groupby([c1, c2, outcome])[['proportion_pop', 'proportion_mod']].sum()) for c1, c2 in it.combinations(columns, 2)])\n",
    "\n",
    "    topline_margins = {\n",
    "        'topline_yes': mod_margins[mod_margins.voting_intent == 'Yes']['proportion'].sum().item(),\n",
    "        'pop_topline_yes': pop_margins[pop_margins.voting_intent == 'Yes']['proportion'].sum().item(),\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'kld': kld,\n",
    "        'kld_1d': kld_1d.mean().item(),\n",
    "        'kld_2d': kld_2d.mean().item(),\n",
    "        'emd': emd,\n",
    "        'emd_1d': emd_1d.mean().item(),\n",
    "        'emd_2d': emd_2d.mean().item(),\n",
    "    } | topline_margins\n",
    "\n",
    "def get_coefs(model_path, posterior=None):\n",
    "    if posterior is None:\n",
    "        posterior = az.from_netcdf(model_path / 'idata.nc').posterior\n",
    "    heckman_coefs = json.load(open(model_path / 'heckman_coefs.json'))\n",
    "\n",
    "    var_names = {\n",
    "        'selection': 'selection',\n",
    "        'outcome': 'outcome',\n",
    "    }\n",
    "\n",
    "    ignore_cols = ['selection_latent']\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for p in ['selection', 'outcome']:\n",
    "        vname = f'{var_names[p]}_%s_effect'\n",
    "        hcoefs = heckman_coefs[p].get('beta', {})\n",
    "\n",
    "        mvals = pd.Series()\n",
    "        rvals = pd.Series()\n",
    "\n",
    "        for c in hcoefs:\n",
    "            cname = c\n",
    "\n",
    "            if ':' in c:\n",
    "                c1, c2 = c.split(':')\n",
    "\n",
    "                if c1 in ignore_cols or c2 in ignore_cols:\n",
    "                    continue\n",
    "\n",
    "                index = []\n",
    "\n",
    "                if vname % f'{c1},{c2}' in posterior:\n",
    "                    c1, c2 = c1, c2\n",
    "\n",
    "                    for cat in hcoefs[c]:\n",
    "                        cat1, cat2 = cat.split(':')\n",
    "                        index.append((cat1, cat2))\n",
    "                elif vname % f'{c2},{c1}' in posterior:\n",
    "                    c1, c2 = c2, c1\n",
    "\n",
    "                    for cat in hcoefs[c]:\n",
    "                        cat2, cat1 = cat.split(':')\n",
    "                        index.append((cat1, cat2))\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                cname = f'{c1}:{c2}'\n",
    "                rvals = pd.Series(hcoefs[c].values(), index=pd.MultiIndex.from_tuples(index, names=[c1, c2])).rename('real')\n",
    "                mvals = posterior[vname % f'{c1},{c2}'].mean(dim=['chain','draw']).to_series().rename('model')\n",
    "            else:\n",
    "                rvals = pd.Series(hcoefs[c]).rename('real')\n",
    "                if vname % c not in posterior: continue\n",
    "                mvals = posterior[vname % c].mean(dim=['chain','draw']).to_series().rename('model')\n",
    "\n",
    "            rvals -= rvals.mean()\n",
    "            mvals -= mvals.mean()\n",
    "\n",
    "            df = pd.concat([rvals, mvals], axis=1)\n",
    "            df['index'] = list(df.index)\n",
    "            df['process'] = p\n",
    "            df['var'] = cname\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            dfs.append(df)\n",
    "\n",
    "    if len(dfs) == 0:\n",
    "        return pd.DataFrame({'process': [], 'var': [], 'index': [], 'real': [], 'model': []})\n",
    "\n",
    "    coefs = pd.concat(dfs)\n",
    "\n",
    "    return coefs[['process', 'var', 'index', 'real', 'model']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869441f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamToLogger:\n",
    "    \"\"\"\n",
    "    Fake file-like stream object that redirects writes to a logger instance.\n",
    "    https://stackoverflow.com/questions/19425736/how-to-redirect-stdout-and-stderr-to-logger-in-python\n",
    "    \"\"\"\n",
    "    def __init__(self, stream, logger, level):\n",
    "        self.stream = stream\n",
    "        self.logger = logger\n",
    "        self.level = level\n",
    "        self.linebuf = ''\n",
    "\n",
    "    def write(self, buf):\n",
    "        for line in buf.rstrip().splitlines():\n",
    "            self.logger.log(self.level, line.rstrip())\n",
    "        self.stream.write(buf)\n",
    "\n",
    "    def flush(self):\n",
    "        self.stream.flush()\n",
    "\n",
    "class CaptureStdStreams:\n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.sys_stdout = sys.stdout\n",
    "        self.sys_stderr = sys.stderr\n",
    "        sys.stdout = self.stdout = StreamToLogger(sys.stdout, self.logger, logging.INFO)\n",
    "        sys.stderr = self.stderr = StreamToLogger(sys.stderr, self.logger, logging.ERROR)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        sys.stdout = self.sys_stdout\n",
    "        sys.stderr = self.sys_stderr\n",
    "\n",
    "class ModelLogger:\n",
    "    def __init__(self, root_logger, mname, mpath):\n",
    "        self.root_logger = root_logger\n",
    "        self.logger = root_logger.getChild(mname)\n",
    "\n",
    "        self.formatter = logging.Formatter(\n",
    "            fmt='[{name}] {asctime} {levelname}: {message}',\n",
    "            datefmt='%m/%d/%Y %H:%M:%S',\n",
    "            style='{'\n",
    "        )\n",
    "\n",
    "        self.fh = logging.FileHandler(mpath / 'log.txt', mode='w')\n",
    "        self.fh.setFormatter(self.formatter)\n",
    "        self.logger.addHandler(self.fh)\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self.logger\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        pass\n",
    "\n",
    "class Timer:\n",
    "    def __enter__(self):\n",
    "        self.start = perf_counter()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.time = perf_counter() - self.start\n",
    "\n",
    "def dict_apply(obj, key, func):\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            if k == key:\n",
    "                obj[k] = func(v)\n",
    "            else:\n",
    "                obj[k] = dict_apply(v, key, func)\n",
    "    elif isinstance(obj, list):\n",
    "        for i, v in enumerate(obj):\n",
    "            obj[i] = dict_apply(v, key, func)\n",
    "    return obj\n",
    "\n",
    "def get_file_hash(filename, root=None):\n",
    "    if root:\n",
    "        file_path = pathlib.Path(root) / filename\n",
    "    else:\n",
    "        file_path = pathlib.Path(filename)\n",
    "\n",
    "    if file_path.suffix == '.json':\n",
    "        meta = json.load(open(file_path, 'r'))\n",
    "        meta = dict_apply(copy.deepcopy(meta), 'file', lambda fn: get_file_hash(fn, file_path.parent))\n",
    "        return hashlib.md5(json.dumps(meta, sort_keys=True).encode('utf-8')).hexdigest()\n",
    "    else:\n",
    "        return hashlib.sha256(open(file_path, 'rb').read()).hexdigest()\n",
    "\n",
    "def get_model_id(model_desc):\n",
    "    desc = copy.deepcopy(model_desc)\n",
    "    # Ignore model name in ID calculation\n",
    "    desc.pop('name')\n",
    "    # Replace file paths with the hashes of the file contents\n",
    "    desc = dict_apply(desc, 'file', get_file_hash)\n",
    "    return hashlib.md5(json.dumps(desc, sort_keys=True).encode('utf-8')).hexdigest()\n",
    "\n",
    "def run_model(model_desc, population_file, model_data, path_prefix='./tmp', root_logger=LOGGER, progress=None, progress_postfix=None):\n",
    "    def aggregate_draws_to_chain_means(draws_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Collapse posterior draws to per-chain means (drops `draw`), keeping cell dims.\n",
    "        if 'draw' not in draws_df.columns or 'chain' not in draws_df.columns or 'N' not in draws_df.columns:\n",
    "            return draws_df\n",
    "\n",
    "        group_cols = [c for c in draws_df.columns if c not in ['N', 'draw']]\n",
    "        if 'N_census' in draws_df.columns:\n",
    "            group_cols_wo_nc = [c for c in group_cols if c != 'N_census']\n",
    "            agg_df = (\n",
    "                draws_df\n",
    "                .groupby(group_cols_wo_nc, observed=False, sort=False)\n",
    "                .agg(N=('N', 'mean'), N_census=('N_census', 'first'))\n",
    "                .reset_index()\n",
    "            )\n",
    "        else:\n",
    "            agg_df = (\n",
    "                draws_df\n",
    "                .groupby(group_cols, observed=False, sort=False)['N']\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "            )\n",
    "\n",
    "        return agg_df\n",
    "\n",
    "    model_id = get_model_id(model_desc)\n",
    "    model_name = f'{model_desc[\"name\"]}-{model_id}'\n",
    "    model_path = pathlib.Path(path_prefix) / model_id\n",
    "    model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    link_path = pathlib.Path(path_prefix) / model_name\n",
    "    if not link_path.is_symlink():\n",
    "        os.symlink(model_path.name, link_path)\n",
    "\n",
    "    if progress:\n",
    "        if progress_postfix:\n",
    "            progress.set_postfix({**progress_postfix, 'model': model_name})\n",
    "        else:\n",
    "            progress.set_postfix({'model': model_name})\n",
    "    else:\n",
    "        print('Model:', model_name)\n",
    "\n",
    "    model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    summary_path = model_path / 'summary.json'\n",
    "\n",
    "    if summary_path.exists():\n",
    "        summary_data = json.load(open(summary_path, 'r'))\n",
    "        # Cleanup legacy idata.nc if present\n",
    "        try:\n",
    "            os.unlink(model_path / 'idata.nc')\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        return {\n",
    "            'model_path': str(model_path),\n",
    "            'model_id': model_id,\n",
    "            'model_name': model_desc['name'],\n",
    "        } | summary_data\n",
    "\n",
    "    # Load or fit model\n",
    "    if (model_path / 'draws.parquet').exists():\n",
    "        mod_df = pd.read_parquet(model_path / 'draws.parquet')\n",
    "        fit_time = float(open(model_path / 'time.txt').read())\n",
    "    else:\n",
    "        with ModelLogger(root_logger, model_name, model_path) as logger, CaptureStdStreams(logger):\n",
    "            try:\n",
    "                with open(model_path / 'model_desc.json', 'w') as f:\n",
    "                    json.dump(model_desc, f)\n",
    "\n",
    "                with Timer() as timer:\n",
    "                    run_result = tm.run_model(model_desc, sample_kwargs=model_desc.get('sample_kwargs'), save_path=str(model_path))\n",
    "                    mod_df = run_result['draws']\n",
    "\n",
    "                root_logger.info(f'Model {model_name} run in {timer.time:.3f} seconds')\n",
    "                fit_time = timer.time\n",
    "\n",
    "                with open(model_path / 'time.txt', 'w') as f:\n",
    "                    f.write(f'{timer.time:.3f}')\n",
    "\n",
    "                with open(model_path / 'model_id.txt', 'w') as f:\n",
    "                    f.write(model_id)\n",
    "\n",
    "                if not (model_path / 'heckman_coefs.json').exists():\n",
    "                    shutil.copyfile(model_data['heckman_coefs.json'], model_path / 'heckman_coefs.json')\n",
    "            except KeyboardInterrupt as e:\n",
    "                raise e\n",
    "            except Exception as e:\n",
    "                root_logger.error(f'Error running model {model_name}: {e}')\n",
    "                traceback.print_exc(file=sys.stderr)\n",
    "                return {\n",
    "                    'model_path': str(model_path),\n",
    "                    'model_id': model_id,\n",
    "                    'model_name': model_desc['name'],\n",
    "                }\n",
    "\n",
    "    model_identifiers = {\n",
    "        'model_path': str(model_path),\n",
    "        'model_id': model_id,\n",
    "        'model_name': model_desc['name'],\n",
    "    }\n",
    "\n",
    "    # Population\n",
    "    pop_cols = ['age_group', 'education', 'gender', 'nationality', 'electoral_district', 'unit', 'voting_intent']\n",
    "    pop_dtype = {col: 'category' for col in pop_cols}\n",
    "    if str(population_file).endswith('.parquet'):\n",
    "        pop_df = pd.read_parquet(population_file)[pop_cols]\n",
    "    else:\n",
    "        pop_df = pd.read_csv(population_file, dtype=pop_dtype)[pop_cols]\n",
    "\n",
    "    # Distances (includes BPV if draw-level is available)\n",
    "    margin_cols = model_desc['input_cols']\n",
    "    distances = get_distances(pop_df, mod_df, margin_cols, 'voting_intent')\n",
    "\n",
    "    # Diagnostics from idata.nc\n",
    "    idata = az.from_netcdf(model_path / 'idata.nc')\n",
    "    sdf = az.summary(idata)\n",
    "    mean_rhat = sdf.r_hat.mean().item()\n",
    "    divergences = idata.sample_stats.diverging.sum().item()\n",
    "\n",
    "    model_fit = {\n",
    "        'fit_time': fit_time,\n",
    "        'mean_rhat': mean_rhat,\n",
    "        'divergences': divergences,\n",
    "    }\n",
    "\n",
    "    posterior = idata.posterior\n",
    "\n",
    "    coefs = get_coefs(model_path, posterior=posterior)\n",
    "    ocoefs = coefs[coefs['process'] == 'outcome']\n",
    "\n",
    "    # Fit robust regression using Huber's T norm\n",
    "    X_with_const = sm.add_constant(ocoefs['real'].values)\n",
    "    y = ocoefs['model'].values\n",
    "    rlm_model = sm.RLM(y, X_with_const, M=sm.robust.norms.HuberT())\n",
    "    rlm_fit = rlm_model.fit()\n",
    "    y_pred = rlm_fit.predict(X_with_const)\n",
    "    mae = np.mean(np.abs(y - y_pred))\n",
    "\n",
    "    model_coefs = {}\n",
    "    if 'rho' in posterior:\n",
    "        model_coefs['sp_slope_vi_mean'] = posterior['rho'].mean(dim=['chain', 'draw']).item()\n",
    "        model_coefs['sp_slope_vi_sd'] = posterior['rho'].std(dim=['chain', 'draw']).item()\n",
    "\n",
    "    rlm_stats = {\n",
    "        'rlm_slope': rlm_fit.params[1],\n",
    "        'rlm_mae': mae,\n",
    "    }\n",
    "\n",
    "    # Persist aggregates for future runs\n",
    "    summary_data = distances | rlm_stats | model_fit | model_coefs\n",
    "    with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Shrink draws.parquet: per-chain means (drop draw)\n",
    "    if 'draw' in mod_df.columns:\n",
    "        agg_df = aggregate_draws_to_chain_means(mod_df)\n",
    "        agg_df.to_parquet(model_path / 'draws.parquet', index=False)\n",
    "\n",
    "    # Remove idata.nc after extracting summary stats\n",
    "    try:\n",
    "        os.unlink(model_path / 'idata.nc')\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    return model_identifiers | summary_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa14abf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DESCRIPTION_TEMPLATE = {\n",
    "    'outcome_col': 'voting_intent',\n",
    "    'population': 'census_data.csv',\n",
    "}\n",
    "\n",
    "def replace_value(obj, orig_value, new_value):\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            obj[k] = replace_value(v, orig_value, new_value)\n",
    "    elif isinstance(obj, list):\n",
    "        for i, v in enumerate(obj):\n",
    "            obj[i] = replace_value(v, orig_value, new_value)\n",
    "    elif obj == orig_value:\n",
    "        obj = new_value\n",
    "    return obj\n",
    "\n",
    "def get_model_description(name, config):\n",
    "    model_desc = copy.deepcopy(MODEL_DESCRIPTION_TEMPLATE)\n",
    "    model_desc['name'] = name\n",
    "    model_desc['model_type'] = config['model_type']\n",
    "    model_desc['input_cols'] = sorted(config.get('input_cols', ['age_group', 'gender', 'education', 'unit', 'nationality']))\n",
    "\n",
    "    if interactions := config.get('interactions') is not None:\n",
    "        model_desc['interactions'] = interactions\n",
    "\n",
    "    if model_desc['model_type'] in ['BP', 'GG', 'PM', 'FS']:\n",
    "        model_desc['survey'] = 'survey_data.csv'\n",
    "\n",
    "    if model_desc['model_type'] in ['EI', 'GG', 'PM', 'FS']:\n",
    "        margin_cols = sorted(config.get('margin_cols', ['unit']))\n",
    "        margin_file_name = f'{\"_\".join(margin_cols)}_margins_data.csv' if len(margin_cols) > 0 else 'margins_data.csv'\n",
    "        model_desc['margin'] = margin_file_name\n",
    "\n",
    "    if config.get('priors_scale_sigma'):\n",
    "        model_desc['priors'] = config.get('priors', {})\n",
    "        model_desc['priors']['scale_sigma'] = config['priors_scale_sigma']\n",
    "\n",
    "    if config.get('sample_kwargs') is not None:\n",
    "        model_desc['sample_kwargs'] = config['sample_kwargs']\n",
    "\n",
    "    if config.get('multilevel') is not None:\n",
    "        model_desc['multilevel'] = config['multilevel']\n",
    "\n",
    "    if config.get('imr') is not None:\n",
    "        model_desc['imr'] = config['imr']\n",
    "\n",
    "    if config.get('centered') is not None:\n",
    "        model_desc['centered'] = config['centered']\n",
    "\n",
    "    if config.get('margin_dist') is not None:\n",
    "        model_desc['margin_dist'] = config['margin_dist']\n",
    "\n",
    "    return model_desc\n",
    "\n",
    "def get_model_data(data_name, margin_cols=[['unit']], tmp_data_prefix='../tmp/data'):\n",
    "    return {\n",
    "        'census_data.csv': '../data/census.csv',\n",
    "        'population.csv': f'{tmp_data_prefix}/{data_name}/population.parquet',\n",
    "        'survey_data.csv': f'{tmp_data_prefix}/{data_name}/estonia_selection.csv',\n",
    "        'margins_data.csv': f'{tmp_data_prefix}/{data_name}/estonia_margins.csv',\n",
    "        'heckman_coefs.json': f'{tmp_data_prefix}/{data_name}/heckman_coefs.json',\n",
    "    } | {\n",
    "        f'{\"_\".join(sorted(cols))}_margins_data.csv': f'{tmp_data_prefix}/{data_name}/estonia_{\"_\".join(sorted(cols))}_margins.csv' for cols in margin_cols\n",
    "    }\n",
    "    \n",
    "def create_symlink(target_path, link_path):\n",
    "    if isinstance(link_path, str):\n",
    "        link_path = pathlib.Path(link_path)\n",
    "\n",
    "    if link_path.is_symlink():\n",
    "        os.unlink(link_path)\n",
    "    os.symlink(target_path, link_path)\n",
    "\n",
    "def get_data_list(file_path, n_seeds_limit=1000):\n",
    "    data_list = json.load(open(file_path, 'r'))\n",
    "    seeds = {data_name: np.unique([data_config['seed'] for _, data_config in data_descs]) for data_name, data_descs in data_list.items()}\n",
    "    return {data_name: [[data_id, data_config] for data_id, data_config in data_descs if data_config['seed'] in seeds[data_name][:n_seeds_limit]] for data_name, data_descs in data_list.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5b6b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data_prefix = '../tmp/data'\n",
    "data_list = get_data_list(f'{tmp_data_prefix}/data_list.json', 10)\n",
    "\n",
    "demography_cols = ['age_group', 'gender', 'education', 'unit', 'nationality', 'electoral_district']\n",
    "input_cols = [col for col in demography_cols if col != 'electoral_district']\n",
    "all_margin_cols = [[col] for col in demography_cols] + [[c1, c2] for c1, c2 in it.combinations(demography_cols, 2)]\n",
    "\n",
    "bp_config = {'model_type': 'BP', 'input_cols': input_cols, 'centered': False }\n",
    "ei_config = {'model_type': 'EI', 'input_cols': input_cols, 'centered': True  }\n",
    "gg_config = {'model_type': 'GG', 'input_cols': input_cols, 'centered': False }\n",
    "pm_config = {'model_type': 'PM', 'input_cols': input_cols, 'centered': True  }\n",
    "fs_config = {'model_type': 'FS', 'input_cols': input_cols, 'centered': True  }\n",
    "\n",
    "common_models = [\n",
    "    get_model_description('1_bp', bp_config),\n",
    "    get_model_description('2_ei', ei_config),\n",
    "    get_model_description('3_gg', gg_config),\n",
    "    get_model_description('4_pm', pm_config),\n",
    "    get_model_description('5_fs', fs_config),\n",
    "]\n",
    "\n",
    "int_models = [\n",
    "    get_model_description('1_int_bp', bp_config | {'interactions': True}),\n",
    "    get_model_description('2_int_ei', ei_config | {'interactions': True}),\n",
    "    get_model_description('3_int_gg', gg_config | {'interactions': True}),\n",
    "    get_model_description('4_int_pm', pm_config | {'interactions': True}),\n",
    "    get_model_description('5_int_fs', fs_config | {'interactions': True}),\n",
    "]\n",
    "\n",
    "def get_margin_model_name(margin_cols):\n",
    "    return 'tpl' if len(margin_cols) == 0 else '_'.join(margin_cols)\n",
    "\n",
    "margin_cols_list = [[], ['unit'], ['electoral_district']]\n",
    "ei_margin_models = [get_model_description(f'2_margin_{get_margin_model_name(margin_cols)}_ei', ei_config | {'input_cols': demography_cols, 'margin_cols': margin_cols}) for margin_cols in margin_cols_list]\n",
    "gg_margin_models = [get_model_description(f'3_margin_{get_margin_model_name(margin_cols)}_gg', gg_config | {'input_cols': demography_cols, 'margin_cols': margin_cols}) for margin_cols in margin_cols_list]\n",
    "pm_margin_models = [get_model_description(f'4_margin_{get_margin_model_name(margin_cols)}_pm', pm_config | {'input_cols': demography_cols, 'margin_cols': margin_cols}) for margin_cols in margin_cols_list]\n",
    "fs_margin_models = [get_model_description(f'5_margin_{get_margin_model_name(margin_cols)}_fs', fs_config | {'input_cols': demography_cols, 'margin_cols': margin_cols}) for margin_cols in margin_cols_list]\n",
    "margin_models = ei_margin_models + gg_margin_models + pm_margin_models + fs_margin_models #+ afs_margin_models + oppm_margin_models + opafs_margin_models + pmdei_margin_models\n",
    "\n",
    "scale_models = [\n",
    "    get_model_description('1_scale_bp', bp_config | {'priors_scale_sigma': 0.5}),\n",
    "    get_model_description('2_scale_ei', ei_config | {'priors_scale_sigma': 0.5}),\n",
    "    get_model_description('3_scale_gg', gg_config | {'priors_scale_sigma': 0.5}),\n",
    "    get_model_description('4_scale_pm', pm_config | {'priors_scale_sigma': 0.5}),\n",
    "    get_model_description('5_scale_fs', fs_config | {'priors_scale_sigma': 0.5}),\n",
    "]\n",
    "\n",
    "experiments = {\n",
    "    'est-default': common_models #+ int_models + scale_models,\n",
    "    'est-electoral-district': margin_models,\n",
    "    'est-no-selection': common_models,\n",
    "    'est-non-response': common_models,\n",
    "    'est-overreport-const': common_models,\n",
    "    'est-heck-cor': common_models,\n",
    "    'est-hcoef-cor': common_models,\n",
    "    'est-hcoef-sigma': common_models,\n",
    "    'est-non-normal-error': common_models,\n",
    "    'est-agg-bias': common_models,\n",
    "    'est-noise': common_models,\n",
    "    'est-sample-size': common_models,\n",
    "    'est-int': common_models + int_models,\n",
    "}\n",
    "\n",
    "model_path_prefix = '../tmp/models'\n",
    "\n",
    "model_results = []\n",
    "\n",
    "model_counts = {exp_name: len(experiments[exp_name]) * len(data_list[exp_name]) for exp_name in experiments}\n",
    "print(model_counts)\n",
    "\n",
    "progress = tqdm(total=sum(model_counts.values()))\n",
    "\n",
    "for experiment_name, model_descriptions in experiments.items():\n",
    "    for data_name, data_desc in data_list[experiment_name]:\n",
    "        model_data = get_model_data(data_name, all_margin_cols)\n",
    "\n",
    "        for desc in model_descriptions:\n",
    "            # Apply data configuration to the model description\n",
    "            model_desc = copy.deepcopy(desc)\n",
    "\n",
    "            for key, value in model_data.items():\n",
    "                model_desc = replace_value(model_desc, key, value)\n",
    "\n",
    "            progress_postfix = {'experiment': experiment_name, 'dataset': data_name, 'description': data_desc}\n",
    "            model_result = run_model(model_desc, model_data['population.csv'], model_data, path_prefix=model_path_prefix, progress=progress, progress_postfix=progress_postfix)\n",
    "            model_result['data_name'] = '-'.join(data_name.split('-')[:-1])\n",
    "            model_result['data_id'] = data_name\n",
    "            model_result['desc'] = ','.join([f'{k.split('/')[-1]}={v}' for k, v in data_desc.items()])\n",
    "            is_success = model_result.get('fit_time') is not None\n",
    "\n",
    "            model_results.append(model_result)\n",
    "\n",
    "            if is_success:\n",
    "                create_symlink(pathlib.Path(model_result['model_path']).name, pathlib.Path(model_path_prefix) / model_desc['name'])\n",
    "\n",
    "            progress.update(1)\n",
    "\n",
    "progress.close()\n",
    "\n",
    "model_results_df = pd.concat([pd.DataFrame(data=result, index=[result['model_path']]) for result in model_results])\n",
    "\n",
    "ignored_cols = ['model_path']\n",
    "ordered_cols = ['model_id', 'data_name', 'data_id', 'desc', 'model_name']\n",
    "other_cols = [col for col in model_results_df.columns if col not in ordered_cols and col not in ignored_cols]\n",
    "\n",
    "model_results_df = model_results_df[ordered_cols + other_cols]\n",
    "model_results_path = pathlib.Path(f'../tmp/models/{datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")}_model_results.csv')\n",
    "model_results_df.to_csv(model_results_path)\n",
    "create_symlink(model_results_path.name, '../tmp/models/model_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
